<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <link rel="alternate icon" type="image/png" href="/img/favicon.png">
    <title>1113Cafe | </title>
    
<link rel="stylesheet" href="/css/reset.css">

    
<link rel="stylesheet" href="/css/style.css">

    
<link rel="stylesheet" href="/css/markdown.css">

    
<link rel="stylesheet" href="/css/fonts.css">

<meta name="generator" content="Hexo 6.3.0"></head>
    <body>
        <div class="paper">
            <div class="paper-main">
                
                    <div class="post-header">
    <a class="logo" href="/">1113Cafe</a>
    <a class="go-home" href="/">
        <svg width="8" height="14" viewBox="0 0 8 14">
            <path d="M7 1L1 7l6 6" stroke="#000" stroke-width="2" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"/>
        </svg>
    </a>
</div>
                
                <div class="post-main">

    
    
        <div class="post-main-title">
            Transformer沿革
        </div>
        <div class="post-meta">
            2023-07-18
        </div>
        <div class="post-md">
            <p>Transformer模型对于语言建模任务的成功非常重要，这一点在大型自然语言处理任务中被广泛证实。Transformer是一种基于自注意力机制的神经网络，它在自然语言处理任务中的表现非常出色，尤其是在长文本序列上的处理效果。自注意力机制使得Transformer能够在序列中找到相关的上下文信息，并将其编码为向量表示。这种技术使得Transformer能够在大规模语料库上进行训练，并学习到更准确的语言表示，从而提高了在自然语言处理任务中的表现。</p>
<p>在语言模型中，Transformer的主要作用是对输入的文本序列进行编码，并将该序列的上下文信息转化为向量表示。使用这种向量表示，我们可以更好地理解文本中的语义和上下文，从而更好地处理自然语言处理任务，如文本分类、机器翻译和文本生成等。</p>
<p>同时，Transformer还对大规模语言预训练模型的发展做出了重要贡献。使用Transformer作为基本的语言模型结构，可以通过大规模的无监督预训练来学习通用的语言表示，从而在各种自然语言处理任务中取得最先进的表现。这些大规模预训练模型，如BERT、GPT-3等，已经在自然语言处理领域取得了重大的突破，并且在不断推动这一领域的发展。因此，可以说Transformer对于LLM的重要性非常巨大，Transformer的历史沿革与LLMs的发展过程一脉相承。</p>
<p>原始Transformer模型的开发动机可以追溯到2014年D. Bahdanau团队对循环神经网络（Recurrent Neural Networks, RNN）的研究，他们扩展了神经机器翻译中的编码器-解码器模型，引入注意力机制，将匹配和翻译联合进行。该模型在翻译过程中每生成一个单词，都会（软）搜索源句子内相关信息最集中的一组位置。然后，该模型根据与这些源位置相关联的上下文向量以及先前生成的所有目标单词来预测目标单词。它不会与基本的编码器-解码器模型一样尝试将整个输入句子编码为单个固定长度向量。反之，它将输入句子编码为向量序列，并在解码翻译时自适应地选择这些向量的子集。这使其不必将源句子的所有信息压缩为固定长度的向量，因此能够更好地处理长句。</p>
<p>但是，该模型仍然存在诸如需要顺序输入连续数据、注意力机制范围较小且不具备记忆性的缺点。因此，谷歌的研究人员在2017年论文“Attention is all you need”中提出Transformer后，Transformers立刻在NLP领域流行起来。与传统的循环神经网络模型不同，Transformer模型使用了全新的架构，即自注意力机制。自注意力机制可以使模型对输入序列的不同位置进行加权，从而更好地捕捉输入序列之间的依赖关系，使模型在处理长序列时更加高效和准确。Transformer模型通常由编码器和解码器两部分组成，可以用于各种序列到序列的任务，如机器翻译、文本摘要、对话生成等。</p>
<p>OpenAI在2017年就尝试过使用RNNs学习智能生成评论并评估文本情绪。随着Transformer的提出，在2018年6月，他们开发出GPT-1（Generative Pre-Training Transformer），GPT-1基于生成式、仅解码器的Transformer架构开发，引入“无监督预训练，监督微调”的概念——此概念时至今日仍被许多Transformer广泛使用。GPT-1建立了GPT系列模型的核心架构，并揭露了其自然语言文本建模的底层原理，即预测下一个单词。</p>
<p>2018年末，谷歌推出BERT（Bidirectional Encoder Representation from Transformers），即Transformers 的双向编码器表示。顾名思义，BERT 是一个双向模型，注意力机制能够关注当前标记的左右两个方向。与传统语言模型不同，它不是在给出所有上文的条件下预测当前最可能的单词，而是随机遮罩一些词，并利用所有没有被遮罩的词进行预测。BERT的创新点主要在于预训练阶段方法，提出了遮罩语言模型（Masked Language Model）和下一句预测（Next Sentence Prediction）的概念。前者可以理解为“完形填空”，随机遮罩语料中的词，用训练模型预测遮罩词；后者是给出两个句子A和B，B只有一半可能性为A的后句，用训练模型预测B是不是A的下一句话。这样的预训练使其既能双向预测，又能分辨句与句之间的关系，但训练收敛慢，硬件资源消耗巨大。</p>
<p>2019年2月，GPT-2问世，遵循与 GPT-1 类似的架构，但规模与训练token数大大增加。GPT-1包括无监督的预训练阶段和有监督的微调阶段，而GPT-2则是完全的无监督预训练模型，略去微调过程，结果可直接用于下游任务，解决了GPT-1时需要针对每个任务微调的局限性与繁琐。</p>
<p>2019年7月，Facebook研究人员推出基于BERT进行改进的RoBERTa。RoBERTa与BERT相较使用更长的预训练周期、更多的训练步数与更大的数据集，改进了原本的训练词遮罩，将其由静态遮罩（Static Masking）变为动态遮罩（Dynamic Masking），并删除下一句预测任务，加快了训练收敛速度。</p>
<p>2019年8月，HuggingFace研究团队也基于BERT，开发出DistilBert。该Transformer致力于缓解随着NLP预训练模型发展，参数量越来越大，受限于算力，实际落地上线困难的问题。在保留97%的性能的前提下，它的模型大小下降40%，推理运算速度加快60%。他们使用G.Hinton在论文“Distilling the Knowledge in a Neural Network”中提出的“知识蒸馏（Knowledge Distilling）”概念，即用一个小的模型去学习一个大模型或一个模型集合的输出，取得了显著的优化成果。</p>
<p>2020年6月，GPT-3由OpenAI推出，模型参数被放大到175B。它正式引入了上下文学习（In-Context Learning, ICL）的概念（在GPT-2的无监督预训练中已有部分应用，但未正式命名）。ICL的关键思想是不对模型参数进行调整，而是给大模型几个示例，大模型就可以从类比中学习。这也意味着，大模型其实并没有经历一个明确的学习过程，而是通过看了一些示例，就出现了解决该领域问题的新能力。ICL对大语言模型能否泛化非常重要。在ICL之前，很多语言模型都是两段式框架，即“无监督预训练，监督微调”，但是在针对下游任务的微调过程中，需要大量的样本参数，否则效果很差。然而标注数据的成本高昂、标注量有限，并且如果数据较少的话，容易导致过拟合，致使模型的泛化能力下降。而ICL这种不需要微调的方法既节省时间与算力资源，还提升了模型性能。以GPT为例，GPT-1是在子任务训练时提供少量的训练样本进行微调，GPT-2是在子任务训练时不提供任何相关的训练样本，直接使用预训练模型在子任务上面做预测，而GPT-3对于所有任务都没有进行任何的梯度更新或微调。GPT-3不仅在各种NLP任务中表现出了非常出色的性能，还在一些专门设计的需要推理或领域适应能力的任务上也表现优秀。OpenAI探索了两种主要方法以进一步改进GPT-3：代码数据训练和人类偏好匹配。前者是为了弥补GPT较为羸弱的代码编写和解决数学问题的能力，训练方法简而言之就是提供给GPT代码和数学问题进行学习；后者则演进为目前热度正高的人类反馈强化学习（Reinforcement Learning with Human Feedback, RLHF），InstructGPT在2022年1月被提出以使用RLHF改进GPT-3。RLHF的工作原理是先训练一个奖励模型，该模型预测人类将如何评价一段文本的质量，再将该奖励模型用于训练LLMs，受训LLM会迭代更新以生成更有可能获得人类高度评价的文本。最近的“Pretraining Language Models with Human Preferences”研究指出，在预训练阶段通过奖励模型纳入人类偏好，可使LLMs生成更符合人类偏好的文本。经过改进的GPT-3模型被OpenAI命名为GPT-3.5。在RLHF的基础上，Anthropic的研究团队在2022年12月又提出了RLAIF（Reinforcement Learning from AI Feedback），正如它的名字，它不再使用人类反馈来生成奖励模型生成评价分数，而是使用另一个人工智能系统——它使用一个规则既定的模型来决定文本输出的优劣以训练一个从文本映射到评价分数的偏好模型，而非传统的奖励模型。</p>
<p>2020年7月，Facebook发布BART，BART吸收了BERT和GPT二者的特点，力图“去其糟粕，取其精华”而成。GPT是一种自回归（Auto-Regressive）的语言模型，可以看作是Transformer架构的解码器部分；BERT是一种自编码（Auto-Encoding）的语言模型，可以看作是Transformer 架构的编码器部分。BERT的优点是获取了双向上下文信息，缺点则是对序列的联合概率作了“独立性假设”，引入了微调阶段不会出现的遮罩，且预训练阶段和生成任务并不一致；GPT的优点是对序列的联合概率未作任何假设，缺点则是单向建模，没有获取双向上下文信息。BART结合二者，建立在标准的Transformer架构上，既有编码器又有解码器，使它比BERT更能胜任文本生成场景，也比GPT多出双向上下文语境信息。</p>
<p>在意识到预训练数据规模和模型规模的扩大对LLM的积极作用后，各大科技巨头都开始利用数据并行性使用更大的语料库训练更大的模型。2021年11月，微软和英伟达推出530B参数MT-NLG模型。2022年4月，谷歌推出540B参数PaLM模型。但同期，DeepMind的研究人员在论文“Training Compute-Optimal Large Language Models”中重新审视了应如何权衡模型规模与训练规模的问题。基于DeepMind估计的计算最优边界，他们推测，一个最优模型应该模型大小比他们先前训练的Gopher模型小4倍，而训练规模应该扩大到先前的4倍多。为了证明这一点，DeepMind训练了一个更优计算的70B模型Chinchilla，该模型使用1.4万亿个token进行训练。评测证明，Chinchilla不仅性能优于模型更大的Gopher，而且其减小的模型尺寸大大降低了推理成本，极大促进了在较小硬件的下游使用。DeepMind的这篇论文是关于LM新的扩展定律非常重要的论文，对LLMs的发展有里程碑式的意义。</p>
<p>2022年11月，OpenAI发布了基于GPT模型（GPT-3.5和GPT-4）的对话模型ChatGPT。ChatGPT使用了RLHF进行训练，同时专门针对对话进行了优化。ChatGPT在与人类交流方面表现出卓越的能力：拥有丰富的知识储备、数学问题推理能力、在多轮对话中准确追踪上下文、以及与人类价值观良好一致以确保安全使用。ChatGPT还在后续支持了插件机制，进一步扩展了ChatGPT与现有工具或应用程序协同合作的能力。到目前为止，它似乎是人工智能历史上最强大的聊天机器人。ChatGPT的推出对未来的人工智能研究产生重大影响，为类人人工智能系统的探索提供了线索。</p>
<p>GPT-4则于2023年3月发布，它的模型规模和训练数据量显著增大，同时将文本输入扩展到多模态信号输入。总体而言，GPT-4解决复杂任务的能力比GPT-3.5更强，在许多评估任务上表现出较大的性能提升，可以更安全地响应恶意或挑衅性查询。</p>

        </div>

    

</div>
                <div class="footer">
    <span>Copyright © 2023 1113Cafe</span>
    <span>Powered By <a target="_blank" href="https://github.com/hexojs/hexo">Hexo</a></span>
</div>


<link rel="stylesheet" href="/css/a11y-dark.min.css">


<script src="/js/highlight.min.js"></script>


<script src="/js/highlightjs-line-numbers.js"></script>


<script>
    hljs.initHighlightingOnLoad();
    hljs.initLineNumbersOnLoad();
</script>

            </div>
        </div>
    </body>
</html>